import vertexai
from vertexai.language_models import TextGenerationModel
from pathlib import Path
import PyPDF2
from google.cloud import storage
import fitz
import json
import io
import base64
import streamlit as st
from PIL import Image
#from src.utils import * - rmoved as everythign is now in the single file 
#from src.vertex import * - removed as everything is now in the single file

#imports from the utils.py
import streamlit as st

#imports from the vertex src
from vertexai.preview.language_models import TextGenerationModel
import vertexai
import streamlit as st
import os

#running utils.py directly here

def reset_session() -> None:
    st.session_state['temperature'] = 0.0
    st.session_state['token_limit'] = 1024
    st.session_state['top_k'] = 1
    st.session_state['top_p'] = 0.1
    st.session_state['debug_mode'] = False
    st.session_state['prompt'] = []
    st.session_state['response'] = []

def hard_reset_session() -> None:
    st.session_state = {states : [] for states in st.session_state}

def create_session_state():
    if 'temperature' not in st.session_state:
        st.session_state['temperature'] = 0.0
    if 'token_limit' not in st.session_state:
        st.session_state['token_limit'] = 1024
    if 'top_k' not in st.session_state:
        st.session_state['top_k'] = 1
    if 'top_p' not in st.session_state:
        st.session_state['top_p'] = 0.1
    if 'debug_mode' not in st.session_state:
        st.session_state['debug_mode'] = False
    if 'prompt' not in st.session_state:
        st.session_state['prompt'] = []
    if 'response' not in st.session_state:
        st.session_state['response'] = []

#running vertex directly here
PROJECT_ID = os.environ.get('GCP_PROJECT') #Your Google Cloud Project ID
LOCATION = os.environ.get('GCP_REGION')   #Your Google Cloud Project Region
vertexai.init(project=PROJECT_ID, location=LOCATION)

@st.cache_resource
def get_model():
    generation_model = TextGenerationModel.from_pretrained("text-bison@001")
    return generation_model

def get_text_generation(prompt="",  **parameters):
    generation_model = get_model()
    response = generation_model.predict(prompt=prompt, **parameters
                                        )
    return response.text

#main app function#

st.set_page_config(
    page_title="Custom-Trained PALM Contract Summarizer",
    page_icon=":robot:",
    layout="centered",
    initial_sidebar_state="expanded",
    menu_items={
        'About': "# This app uses a version of the Vertex PaLM mode, fine-tuned on the Atticus Project Contract Understanding Atticus Dataset, to summarize contracts submitted as PDFs."
    }
)

#creating session states
create_session_state()

image = Image.open('./image/palm.jpg')
st.image(image)
st.title(":red[ASL Team 2] PaLM 2 Contract Summarizer Demo")

with st.sidebar:
    image = Image.open('./image/sidebar_image.jpg')
    st.image(image)
    st.markdown("<h2 style='text-align: center; color: red;'>Setting Tab</h2>", unsafe_allow_html=True)

    st.write("Model Settings:")

    #define the temeperature for the model
    temperature_value = st.slider('Temperature :', 0.0, 1.0, 0.2)
    st.session_state['temperature'] = temperature_value

    #define the temeperature for the model
    token_limit_value = st.slider('Token limit :', 1, 1024, 256)
    st.session_state['token_limit'] = token_limit_value

    #define the temeperature for the model
    top_k_value = st.slider('Top-K  :', 1,40,40)
    st.session_state['top_k'] = top_k_value

    #define the temeperature for the model
    top_p_value = st.slider('Top-P :', 0.0, 1.0, 0.8)
    st.session_state['top_p'] = top_p_value

    if st.button("Reset Session"):
        reset_session()

#defining extract text

def extract_text_from_pdf(uploaded_file):
    doc = fitz.open(uploaded_file)
    text = ""
    for page_num in range(doc.page_count):
        page = doc[page_num]
        text += page.get_text()
    return text

#defining prompt template
prompt_template = """
 
You are a lawyer in a corporate legal department.
Consider the following contract:
<begin contract text>     
```{text}```
<end contract text>
 
FIRST, you must create a one-sentence summary of the contract.
 
SECOND, you must list the following specific information from the contract:
	1) the begin date of the contract
	2) the end date of the contract
	3) the names the human signatories to the contract
THIRD, If this contract is a sales contract, then you must also attentively extract the following additional specific information from the contract:
	1) vendor company name
	2) customer company name
	3) each contract term that pertains to contract cancellation and/or contract renewal (especially automatic renewal)
	4) each contract term that pertains to billing and/or payment
	5) each contract line item (each to include [product description], [product quantity], [product price, with currency if specified], and [charge periodicity (one-time, daily, weekly, monthly, annual, etc.)] for each line item)
	6) each charge not included in the line items (including currency if specified)
 
"""

with st.container():
    st.write("Current Generator Settings: ")
    # if st.session_state['temperature'] or st.session_state['debug_mode'] or :
    st.write ("Temperature: ",st.session_state['temperature']," \t \t Token limit: ",st.session_state['token_limit']
                ," \t \t Top-K: ",st.session_state['top_k']
                ," \t \t Top-P: ",st.session_state['top_p']
                ," \t \t Debug Model: ",st.session_state['debug_mode'])


    uploaded_file = st.file_uploader("Choose a file", type='pdf')
    if uploaded_file:
        st.markdown("<h3 style='text-align: center; color: red;'>Generator Model Response</h3>", unsafe_allow_html=True)
        with st.spinner('Custom PaLM model is working to generate, wait.....'):
            doc = fitz.open("pdf",uploaded_file)
            text =""
            for page_num in range (doc.page_count):
                page = doc[page_num]
                text += page.get_text()
            #text = extract_text_from_pdf(uploaded_file)
            #exception handling for length needed
            prompt = prompt_template.format(text=text)
            response = get_text_generation(prompt=prompt, temperature = st.session_state['temperature'],
                            max_output_tokens = st.session_state['token_limit'],
                            top_p = st.session_state['top_p'],
                            top_k = st.session_state['top_k'])
            st.session_state['response'].append(response)
            st.markdown(response) 



    #prompt = st.text_area("Add your prompt: ",height = 100)
    #if prompt:
    #    st.session_state['prompt'].append(prompt)
    #    st.markdown("<h3 style='text-align: center; color: blue;'>Generator Model Response</h3>", unsafe_allow_html=True)
    #    with st.spinner('PaLM is working to generate, wait.....'):
    #        response = get_text_generation(prompt=prompt, temperature = st.session_state['temperature'],
    #                            max_output_tokens = st.session_state['token_limit'],
    #                            top_p = st.session_state['top_p'],
    #                            top_k = st.session_state['top_k'])
    #        st.session_state['response'].append(response)
    #        st.markdown(response)
